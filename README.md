# ğŸ•·ï¸ Books to Scrape â€“ Web Scraper (Scrapy)

A robust, fully automated **web scraping project** built with **Scrapy** to extract book data from the website [Books to Scrape](https://books.toscrape.com).  

This spider crawls all book listings, page by page, and exports a clean dataset (`books_info.csv`) containing titles, prices, ratings, availability, and cover image URLs â€” ready for data analysis or visualization in the accompanying **Streamlit Books Dashboard**.

---

## ğŸ§  Project Overview

This project demonstrates **end-to-end web data extraction** using Pythonâ€™s `Scrapy` framework.  
It automatically crawls every page of the â€œBooks to Scrapeâ€ catalog, normalizes the data, and outputs a structured CSV file suitable for analytics, reporting, or dashboards.

**Key Goals:**
- Automate data collection from a paginated e-commerce-like site.
- Export structured book data into a clean, ready-to-use format.
- Form the backend data source for an interactive Streamlit dashboard.

---

## ğŸ“¦ Features

| Feature | Description |
|----------|-------------|
| ğŸ•·ï¸ **Automated Crawling** | Crawls every page of the Books to Scrape catalog. |
| ğŸ“˜ **Data Extraction** | Extracts book title, price, rating, stock availability, and image URL. |
| ğŸ” **Pagination Handling** | Automatically follows â€œNextâ€ page links until all books are scraped. |
| ğŸ’¾ **Clean CSV Export** | Outputs data to `books_info.csv` with UTF-8 encoding for Excel/Streamlit compatibility. |
| ğŸ§© **Plug-and-Play** | The resulting CSV works directly with the [Books Dashboard (Streamlit)](../streamlit_books_dashboard.py). |
| ğŸ§± **Lightweight & Portable** | Single-file script â€” no project scaffolding needed. |

---

## âš™ï¸ How It Works

### 1ï¸âƒ£ Spider Definition
```python
class BooksSpider(scrapy.Spider):
    name = "books"
    start_urls = ["https://books.toscrape.com/catalogue/page-1.html"]
```
The spider starts from the first catalog page and parses all book cards (article.product_pod).

---

### 2ï¸âƒ£ Data Extraction

For each book, the spider collects:


| Field            | Selector                       | Description                                        |
| ---------------- | ------------------------------ | -------------------------------------------------- |
| **Book Title**   | `h3 a::attr(title)`            | The title of the book                              |
| **Book Price**   | `p.price_color::text`          | Price including the currency symbol                |
| **Availability** | `p.instock.availability::text` | In-stock or out-of-stock info                      |
| **Rating**       | `p.star-rating::attr(class)`   | Extracted as a textual rating (â€œOneâ€, â€œTwoâ€, etc.) |
| **Image URL**    | `img::attr(src)`               | Full image URL (auto-joined with site base)        |

Each book is yielded as a structured Python dictionary, which Scrapy automatically converts to CSV.

---

### 3ï¸âƒ£ Pagination

next_page = response.css("li.next a::attr(href)").get()
if next_page:
    yield response.follow(next_page, callback=self.parse)

The spider detects the â€œNextâ€ button on each page and recursively crawls until no further pages exist (all 1000 books).

---

### 4ï¸âƒ£ Output Configuration

custom_settings = {
    "FEEDS": {
        "books_info.csv": {
            "format": "csv",
            "overwrite": True,
            "encoding": "utf-8-sig",
        },
    },
    "LOG_LEVEL": "INFO",
}

* The spider saves all results into books_info.csv.
* UTF-8 encoding ensures compatibility with Excel, Streamlit, and Power BI.
* Clean logs (INFO level) make it suitable for client or production environments.


5ï¸âƒ£ Standalone Execution

This spider can run without the Scrapy CLI using:

if __name__ == "__main__":
    process = CrawlerProcess()
    process.crawl(BooksSpider)
    process.start()

So you can launch it simply with:

python books_spider.py

ğŸ“Š Sample Output

After completion, youâ€™ll find a file books_info.csv like this:

| Book Title           | Book Price | Instock Availability    | Rating | Image URL                                                     |
| -------------------- | ---------- | ----------------------- | ------ | ------------------------------------------------------------- |
| A Light in the Attic | Â£51.77     | In stock (22 available) | Three  | [https://books.toscrape.com/](https://books.toscrape.com/)... |
| Tipping the Velvet   | Â£53.74     | In stock (20 available) | One    | [https://books.toscrape.com/](https://books.toscrape.com/)... |


This file can be used directly in your Streamlit dashboard or any data visualization tool.

ğŸ§° Tech Stack

| Purpose                      | Tool                                           |
| ---------------------------- | ---------------------------------------------- |
| **Web Scraping**             | [Scrapy](https://scrapy.org)                   |
| **Language**                 | Python 3.8+                                    |
| **Data Format**              | CSV (UTF-8)                                    |
| **Visualization (optional)** | Streamlit + Plotly (for dashboard integration) |


âš™ï¸ Installation & Usage
1ï¸âƒ£ Clone this repository

git clone https://github.com/<your-username>/books-to-scrape-scraper.git
cd books-to-scrape-scraper

2ï¸âƒ£ Install dependencies

pip install scrapy

3ï¸âƒ£ Run the spider

python books_spider.py

4ï¸âƒ£ View output

A file named books_info.csv will appear in the same directory.

ğŸ§  Integration with Streamlit Dashboard

This scraper is part of a two-step data project:

1.    Data Extraction: (This script) Scrape and save books_info.csv.

2.    Data Visualization: Use the companion app streamlit_books_dashboard.py to interactively explore the dataset.

You can find the Streamlit dashboard version here ğŸ‘‰ Books Dashboard Repo
 (replace with your actual link).

ğŸ§¾ License

This project is released under the MIT License.
Youâ€™re free to use, modify, or build upon it with proper credit.

ğŸ‘¨â€ğŸ’» Author

Onyekachi Ejimofor
Python Developer | Web Scraping & Automation | Data Visualization
ğŸ“§ Email: onyeife@gmail.com
ğŸŒ GitHub: https://github.com/Onyeksman
ğŸ’¼ LinkedIn: www.linkedin.com/in/onyekachiejimofor
